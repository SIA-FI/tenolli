{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos bibliotecas\n",
    "Descargamos la biblioteca de transformers de keras, sacreblue para la evaluación y tqdm para ver el avance de la evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11445,
     "status": "ok",
     "timestamp": 1642572055844,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "Y8EEc3gEJMAu",
    "outputId": "90eda7f1-733e-4a95-a172-5ed309010109"
   },
   "outputs": [],
   "source": [
    "pip install keras-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 927 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2022.1.18-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 316 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (1.21.3)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "\u001b[31mERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 520, in _determine_conflicts\n",
      "    return check_install_conflicts(to_install)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 108, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 50, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1895, in get_metadata\n",
      "    raise KeyError(\"No metadata except PKG-INFO is available\")\n",
      "KeyError: 'No metadata except PKG-INFO is available'\u001b[0m\n",
      "Installing collected packages: colorama, regex, tabulate, portalocker, sacrebleu\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script sacrebleu is installed in '/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed colorama-0.4.4 portalocker-2.3.2 regex-2022.1.18 sacrebleu-2.0.0 tabulate-0.8.9\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 737 kB/s eta 0:00:01\n",
      "\u001b[31mERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 520, in _determine_conflicts\n",
      "    return check_install_conflicts(to_install)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 108, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 50, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1895, in get_metadata\n",
      "    raise KeyError(\"No metadata except PKG-INFO is available\")\n",
      "KeyError: 'No metadata except PKG-INFO is available'\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed tqdm-4.62.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos\n",
    "Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19876,
     "status": "ok",
     "timestamp": 1642572090629,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "yRFtYZsHJbWe",
    "outputId": "b33f0af2-d74e-4dfe-ba8f-4044549552c8"
   },
   "outputs": [],
   "source": [
    "# Biblioteca para usar transformers\n",
    "from keras_transformer import get_model, decode\n",
    "from pickle import load\n",
    "\n",
    "# Ponemos la semilla en 0 para replicar\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1642572439467,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "wNBg2MqMJgXO"
   },
   "outputs": [],
   "source": [
    "# Cargamos el archivo con los tokens paralelos\n",
    "filenameParalelo = 'corpus/tokens_paralelos.txt'\n",
    "with open(filenameParalelo) as file:\n",
    "    datos = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separarCorpus(corpus):\n",
    "    \"\"\"\n",
    "    Nos permite leer el archivo de tokens y separar de manera\n",
    "    ordenada para tener el corpus en español y nahuatl\n",
    "    \n",
    "    Parámetros\n",
    "    corpus: Lista de oraciones paralelas\n",
    "    \n",
    "    return lista de oraciones en nahuatl y en español\n",
    "    \"\"\"\n",
    "    nah = list()\n",
    "    esp = list()\n",
    "    for oracion in corpus:\n",
    "        if oracion != \"\\t\\n\":\n",
    "            sep = oracion[:-1].split(\"\\t\")\n",
    "            nah.append(sep[0].split(\" \"))\n",
    "            esp.append(sep[1].split(\" \"))\n",
    "    return nah, esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "nah, esp = separarCorpus(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "LIMITE = 512\n",
    "nah = list(map(lambda x: x[:LIMITE], nah))\n",
    "esp = list(map(lambda x: x[:LIMITE], esp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nah[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1642572469929,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "IlkbG3p3Kcfa"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_dict():\n",
    "    \"\"\"\n",
    "    Crea un diccionario cuya llave por default de \n",
    "    elementos nuevos, es el tamaño del diccionario\n",
    "    \n",
    "    return defaultdict\n",
    "    \"\"\"\n",
    "    token_dict = defaultdict()\n",
    "    token_dict.default_factory = lambda: len(token_dict)\n",
    "    return token_dict\n",
    "\n",
    "# Variables globales\n",
    "PAD = 0\n",
    "START = 1\n",
    "END = 2\n",
    "\n",
    "def build_token_dict(token_list):\n",
    "    \"\"\"\n",
    "    Crea el diccionario para un vocabulario\n",
    "    \n",
    "    Parámetros\n",
    "    token_list: Lista de oraciones tokenizadas\n",
    "    \n",
    "    return diccionario de tokens y su mapeo a numeros\n",
    "    \"\"\"\n",
    "    token_dict = create_dict()\n",
    "    token_dict['<PAD>'] = PAD\n",
    "    token_dict['<START>'] = START\n",
    "    token_dict['<END>'] = END\n",
    "\n",
    "    for tokens in token_list:\n",
    "        for token in tokens:\n",
    "          if token not in token_dict:\n",
    "            token_dict[token]\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "source_tokens = nah\n",
    "target_tokens = esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "source_token_dict = build_token_dict(source_tokens)                     #Nahualt\n",
    "target_token_dict = build_token_dict(target_tokens)                     #Español\n",
    "target_token_dict_inv = {v:k for k,v in target_token_dict.items()}      #Inverso para la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario del source para decodificar\n",
    "source_token_dict_inv = {v:k for k,v in source_token_dict.items()}      #Inverso para la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 1510,
     "status": "ok",
     "timestamp": 1642572473528,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "tADm6ra3NOUQ"
   },
   "outputs": [],
   "source": [
    "# Agregar start y end a cada frase del set de entrenamiento\n",
    "encoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
    "decoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
    "output_tokens = [tokens + ['<END>'] for tokens in target_tokens]\n",
    "\n",
    "source_max_len = max(map(len, encoder_tokens))      #Maximo de palabras en frase de ingles\n",
    "target_max_len = max(map(len, decoder_tokens))      #Maximo de palabras en frase de español\n",
    "\n",
    "#Llenar con padding las frases para que sean del mismo tamaño\n",
    "encoder_tokens = [tokens + ['<PAD>']*(source_max_len-len(tokens)) for tokens in encoder_tokens]\n",
    "decoder_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in decoder_tokens]\n",
    "output_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in output_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1642572474439,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "TKMzFwMpPtFo",
    "outputId": "5d888ed5-33bc-40c8-fe4a-8aeb3093819b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'no@@', 'n', 'tla@@', 'ka@@', 'h', 'ma@@', 'ya@@', 'ki@@', 'ah', 'amo', 'ki@@', 'x@@', 'to@@', 'no@@', 'n', 'no@@', 'n', 'in', 'tz@@', 'o@@', 'tz@@', 'o@@', 'ma', 'cui@@', 'ka', 'in', 'pan@@', 'ta@@', 'l@@', 'on', 'chi@@', 'pa@@', 'hua@@', 'k', 'on@@', 'tz@@', 'o', 'in@@', 'ka@@', 'z@@', 'on', 'de', 'man@@', 'ta', 'ko@@', 'to@@', 'n@@', 'tle@@', 'h', 'chi@@', 'pa@@', 'hua@@', 'k', 'o', 'zan@@', 'tle@@', 'li@@', 'hui@@', 'z', 'in', 'xo@@', 'm@@', 'p@@', 'le@@', 'lo@@', 'h', 'tla', 'cue@@', 'cue@@', 'zo@@', 'tl', 'ka', 'zo@@', 'ya@@', 'tl', 'huan', 'in', 'te@@', 'ka@@', 'k', 'de', 'cue@@', 'ro', 'ki@@', 'on', 'o@@', 'ma@@', 'ya@@', 'ki@@', 'aya@@', 'h', 'no@@', 'n', 'i@@', 'hue@@', 'ka', 'tla@@', 'ka@@', 'a@@', 'x@@', 'k@@', 'an', 'no@@', 'n', 'in', 'tz@@', 'o@@', 'tz@@', 'o@@', 'ma@@', 'huan', 'yo@@', 'po@@', 'li@@', 'ke@@', 'h@@', 'a@@', 'x@@', 'k@@', 'an', 'mo', 'cue@@', 'h', 'za', 'no@@', 'n', 'xo@@', 'm@@', 'p@@', 'le@@', 'lo@@', 'h', 'ma@@', 'z@@', 'ke', 'ma@@', 'ki@@', 'a', 'no@@', 'n', 'tla@@', 'chi@@', 'chi@@', 'ke@@', 'h', 'ka', 'o@@', 'i@@', 'k@@', 'ze', 'ma@@', 'te@@', 'r@@', 'ia@@', 'tl', 'za@@', 'huetl@@', 'ze@@', 'ke', 'ki', 'cue', 'no@@', 'n', 'zo@@', 'ya@@', 'tl', 'huan', 'no@@', 'n', 'in', 'te@@', 'ka@@', 'k', 'tla@@', 'chi@@', 'chi@@', 'ke@@', 'h', 'ka', 'cui@@', 'tla@@', 'x@@', 'tle@@', 'k', 'mi@@', 'e@@', 'k', 'tla@@', 'ka', 'a@@', 'ye@@', 'k@@', 'mo', 'mo', 'te@@', 'ka@@', 'k@@', 'tia', 'ka', 'te@@', 'ka@@', 'k@@', 'tle', 'a@@', 'x@@', 'k@@', 'an', 'mo', 'te@@', 'ka@@', 'k@@', 'ti@@', 'ah', 'ka', 'za@@', 'pa@@', 'to@@', 'z', 'huan', 'b@@', 'o@@', 'ta@@', 'z', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print(encoder_tokens[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28174,
     "status": "ok",
     "timestamp": 1642572504560,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "_MPmN7PwP6ZN",
    "outputId": "8d2d2239-5f3f-4989-8904-acf91a250b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 18, 80, 58, 106, 60, 130, 18, 100, 78, 33, 15, 43, 130, 22, 117, 78, 33, 15, 82, 205, 169, 125, 82, 106, 106, 60, 154, 227, 52, 183, 238, 140, 82, 125, 82, 161, 15, 133, 128, 18, 20, 36, 248, 29, 7, 53, 9, 106, 148, 37, 140, 18, 71, 146, 36, 16, 20, 65, 106, 148, 65, 136, 32, 82, 231, 7, 128, 7, 98, 148, 37, 140, 18, 56, 42, 15, 82, 205, 36, 16, 20, 65, 106, 148, 164, 36, 16, 119, 130, 58, 51, 107, 65, 243, 98, 128, 194, 37, 140, 18, 56, 42, 231, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Se mapean a numeros los tokens de cada oración\n",
    "encoder_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encoder_tokens]\n",
    "decoder_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decoder_tokens]\n",
    "output_decoded = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]\n",
    "\n",
    "print(encoder_input[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable global\n",
    "test = int(len(nah)*0.9)\n",
    "\n",
    "# Dividimos en conjunto de test y train\n",
    "encoder_input_test = encoder_input[test:]\n",
    "decoder_input_test = decoder_input[test:]\n",
    "output_decoded_test = output_decoded[test:]\n",
    "\n",
    "encoder_input = encoder_input[:test]\n",
    "decoder_input = decoder_input[:test]\n",
    "output_decoded = output_decoded[:test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3752"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33764"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración del entorno\n",
    "Comandos para poder trabajar con ciertas gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[5], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUDA_DEVICE_ORDER=\"PCI_BUS_ID\"\n",
    "# %env CUDA_VISIBLE_DEVICES=\"5\"\n",
    "# ! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable global\n",
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1642572870335,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "pLtCpkwXRm3z",
    "outputId": "1d545db4-0cf6-4fd8-b1df-7b772a9e9a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Encoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Encoder-Token-Embedding (Embed  [(None, None, 32),  14048       ['Encoder-Input[0][0]']          \n",
      " dingRet)                        (439, 32)]                                                       \n",
      "                                                                                                  \n",
      " Encoder-Embedding (TrigPosEmbe  (None, None, 32)    0           ['Encoder-Token-Embedding[0][0]']\n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Encoder-Embedding[0][0]']      \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-Embedding[0][0]',      \n",
      " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    64          ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward (FeedFor  (None, None, 32)    8352        ['Encoder-1-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Dropout   (None, None, 32)    0           ['Encoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Add (Add  (None, None, 32)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Norm (La  (None, None, 32)    64          ['Encoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Encoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Decoder-Token-Embedding (Embed  [(None, None, 32),  14048       ['Decoder-Input[0][0]']          \n",
      " dingRet)                        (439, 32)]                                                       \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Decoder-Embedding (TrigPosEmbe  (None, None, 32)    0           ['Decoder-Token-Embedding[0][0]']\n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    64          ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Decoder-Embedding[0][0]']      \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward (FeedFor  (None, None, 32)    8352        ['Encoder-2-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Dropout   (None, None, 32)    0           ['Encoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-Embedding[0][0]',      \n",
      " on-Add (Add)                                                     'Decoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Add (Add  (None, None, 32)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    64          ['Decoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Norm (La  (None, None, 32)    64          ['Encoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    4224        ['Decoder-1-MultiHeadSelfAttentio\n",
      " ion (MultiHeadAttention)                                        n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-1-MultiHeadQueryAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-1-MultiHeadSelfAttentio\n",
      " ion-Add (Add)                                                   n-Norm[0][0]',                   \n",
      "                                                                  'Decoder-1-MultiHeadQueryAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    64          ['Decoder-1-MultiHeadQueryAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward (FeedFor  (None, None, 32)    8352        ['Decoder-1-MultiHeadQueryAttenti\n",
      " ward)                                                           on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward-Dropout   (None, None, 32)    0           ['Decoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward-Add (Add  (None, None, 32)    0           ['Decoder-1-MultiHeadQueryAttenti\n",
      " )                                                               on-Norm[0][0]',                  \n",
      "                                                                  'Decoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Decoder-1-FeedForward-Norm (La  (None, None, 32)    64          ['Decoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Decoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Decoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    64          ['Decoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    4224        ['Decoder-2-MultiHeadSelfAttentio\n",
      " ion (MultiHeadAttention)                                        n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'Encoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-2-MultiHeadQueryAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-2-MultiHeadSelfAttentio\n",
      " ion-Add (Add)                                                   n-Norm[0][0]',                   \n",
      "                                                                  'Decoder-2-MultiHeadQueryAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    64          ['Decoder-2-MultiHeadQueryAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward (FeedFor  (None, None, 32)    8352        ['Decoder-2-MultiHeadQueryAttenti\n",
      " ward)                                                           on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward-Dropout   (None, None, 32)    0           ['Decoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward-Add (Add  (None, None, 32)    0           ['Decoder-2-MultiHeadQueryAttenti\n",
      " )                                                               on-Norm[0][0]',                  \n",
      "                                                                  'Decoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Decoder-2-FeedForward-Norm (La  (None, None, 32)    64          ['Decoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Decoder-Output (EmbeddingSim)  (None, None, 439)    439         ['Decoder-2-FeedForward-Norm[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'Decoder-Token-Embedding[0][1]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 87,927\n",
      "Trainable params: 87,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Crear la red transformer\n",
    "model = get_model(\n",
    "    token_num = max(len(source_token_dict),len(target_token_dict)),\n",
    "    embed_dim = 32,\n",
    "    encoder_num = 2,\n",
    "    decoder_num = 2,\n",
    "    head_num = 4,\n",
    "    hidden_dim = 128,\n",
    "    dropout_rate = 0.05,\n",
    "    use_same_embed = False,\n",
    ")\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tjvERBYTPMS",
    "outputId": "613aaf8b-a195-4cbf-8f68-3c472f6b3c32",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 258\n",
      "1056/1056 [==============================] - 88s 77ms/step - loss: 0.1786\n",
      "Epoca: 259\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1786\n",
      "Epoca: 260\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1786\n",
      "Epoca: 261\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1786\n",
      "Epoca: 262\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1785\n",
      "Epoca: 263\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1786\n",
      "Epoca: 264\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1785\n",
      "Epoca: 265\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1785\n",
      "Epoca: 266\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1784\n",
      "Epoca: 267\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1785\n",
      "Epoca: 268\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1784\n",
      "Epoca: 269\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1784\n",
      "Epoca: 270\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1784\n",
      "Epoca: 271\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1784\n",
      "Epoca: 272\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1784\n",
      "Epoca: 273\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1784\n",
      "Epoca: 274\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1783\n",
      "Epoca: 275\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1783\n",
      "Epoca: 276\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1783\n",
      "Epoca: 277\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1783\n",
      "Epoca: 278\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1783\n",
      "Epoca: 279\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1783\n",
      "Epoca: 280\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1782\n",
      "Epoca: 281\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1782\n",
      "Epoca: 282\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1782\n",
      "Epoca: 283\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1782\n",
      "Epoca: 284\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1782\n",
      "Epoca: 285\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1781\n",
      "Epoca: 286\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1782\n",
      "Epoca: 287\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1782\n",
      "Epoca: 288\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1780\n",
      "Epoca: 289\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1781\n",
      "Epoca: 290\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1780\n",
      "Epoca: 291\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1780\n",
      "Epoca: 292\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1780\n",
      "Epoca: 293\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1781\n",
      "Epoca: 294\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1780\n",
      "Epoca: 295\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1780\n",
      "Epoca: 296\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1780\n",
      "Epoca: 297\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1779\n",
      "Epoca: 298\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1780\n",
      "Epoca: 299\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1780\n",
      "Epoca: 300\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1779\n",
      "Epoca: 301\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1779\n",
      "Epoca: 302\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1778\n",
      "Epoca: 303\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1778\n",
      "Epoca: 304\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1778\n",
      "Epoca: 305\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1778\n",
      "Epoca: 306\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1778\n",
      "Epoca: 307\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1778\n",
      "Epoca: 308\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1778\n",
      "Epoca: 309\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1778\n",
      "Epoca: 310\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1778\n",
      "Epoca: 311\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1777\n",
      "Epoca: 312\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1778\n",
      "Epoca: 313\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1777\n",
      "Epoca: 314\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1777\n",
      "Epoca: 315\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1776\n",
      "Epoca: 316\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1777\n",
      "Epoca: 317\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1776\n",
      "Epoca: 318\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1777\n",
      "Epoca: 319\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1776\n",
      "Epoca: 320\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1776\n",
      "Epoca: 321\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1776\n",
      "Epoca: 322\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1776\n",
      "Epoca: 323\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1776\n",
      "Epoca: 324\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1776\n",
      "Epoca: 325\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1776\n",
      "Epoca: 326\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1776\n",
      "Epoca: 327\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1776\n",
      "Epoca: 328\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1775\n",
      "Epoca: 329\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1776\n",
      "Epoca: 330\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1774\n",
      "Epoca: 331\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1775\n",
      "Epoca: 332\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1775\n",
      "Epoca: 333\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1774\n",
      "Epoca: 334\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1774\n",
      "Epoca: 335\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1774\n",
      "Epoca: 336\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1774\n",
      "Epoca: 337\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1774\n",
      "Epoca: 338\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1774\n",
      "Epoca: 339\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1774\n",
      "Epoca: 340\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1773\n",
      "Epoca: 341\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1773\n",
      "Epoca: 342\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1773\n",
      "Epoca: 343\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1772\n",
      "Epoca: 344\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1773\n",
      "Epoca: 345\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1772\n",
      "Epoca: 346\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1773\n",
      "Epoca: 347\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1773\n",
      "Epoca: 348\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1772\n",
      "Epoca: 349\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1773\n",
      "Epoca: 350\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1772\n",
      "Epoca: 351\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1772\n",
      "Epoca: 352\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1772\n",
      "Epoca: 353\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1772\n",
      "Epoca: 354\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1771\n",
      "Epoca: 355\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1771\n",
      "Epoca: 356\n",
      "1056/1056 [==============================] - 81s 77ms/step - loss: 0.1771\n",
      "Epoca: 357\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1771\n",
      "Epoca: 358\n",
      "1056/1056 [==============================] - 82s 77ms/step - loss: 0.1771\n",
      "Epoca: 359\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1771\n",
      "Epoca: 360\n",
      "1056/1056 [==============================] - 84s 80ms/step - loss: 0.1772\n",
      "Epoca: 361\n",
      "1056/1056 [==============================] - 84s 79ms/step - loss: 0.1771\n",
      "Epoca: 362\n",
      "1056/1056 [==============================] - 84s 79ms/step - loss: 0.1772\n",
      "Epoca: 363\n",
      "1056/1056 [==============================] - 84s 79ms/step - loss: 0.1771\n",
      "Epoca: 364\n",
      "1056/1056 [==============================] - 84s 79ms/step - loss: 0.1770\n",
      "Epoca: 365\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1770\n",
      "Epoca: 366\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1771\n",
      "Epoca: 367\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1770\n",
      "Epoca: 368\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1771\n",
      "Epoca: 369\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1770\n",
      "Epoca: 370\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1770\n",
      "Epoca: 371\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1770\n",
      "Epoca: 372\n",
      "1056/1056 [==============================] - 85s 81ms/step - loss: 0.1770\n",
      "Epoca: 373\n",
      "1056/1056 [==============================] - 86s 82ms/step - loss: 0.1770\n",
      "Epoca: 374\n",
      "1056/1056 [==============================] - 87s 83ms/step - loss: 0.1770\n",
      "Epoca: 375\n",
      "1056/1056 [==============================] - 84s 80ms/step - loss: 0.1770\n",
      "Epoca: 376\n",
      "1056/1056 [==============================] - 89s 84ms/step - loss: 0.1769\n",
      "Epoca: 377\n",
      "1056/1056 [==============================] - 89s 84ms/step - loss: 0.1770\n",
      "Epoca: 378\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1769\n",
      "Epoca: 379\n",
      "1056/1056 [==============================] - 85s 81ms/step - loss: 0.1769\n",
      "Epoca: 380\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1769\n",
      "Epoca: 381\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1769\n",
      "Epoca: 382\n",
      "1056/1056 [==============================] - 87s 83ms/step - loss: 0.1768\n",
      "Epoca: 383\n",
      "1056/1056 [==============================] - 84s 80ms/step - loss: 0.1769\n",
      "Epoca: 384\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1768\n",
      "Epoca: 385\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1768\n",
      "Epoca: 386\n",
      "1056/1056 [==============================] - 86s 82ms/step - loss: 0.1769\n",
      "Epoca: 387\n",
      "1056/1056 [==============================] - 85s 81ms/step - loss: 0.1768\n",
      "Epoca: 388\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1768\n",
      "Epoca: 389\n",
      "1056/1056 [==============================] - 84s 79ms/step - loss: 0.1768\n",
      "Epoca: 390\n",
      "1056/1056 [==============================] - 87s 82ms/step - loss: 0.1768\n",
      "Epoca: 391\n",
      "1056/1056 [==============================] - 87s 83ms/step - loss: 0.1768\n",
      "Epoca: 392\n",
      "1056/1056 [==============================] - 85s 81ms/step - loss: 0.1768\n",
      "Epoca: 393\n",
      "1056/1056 [==============================] - 91s 86ms/step - loss: 0.1768\n",
      "Epoca: 394\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1767\n",
      "Epoca: 395\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1767\n",
      "Epoca: 396\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1767\n",
      "Epoca: 397\n",
      "1056/1056 [==============================] - 90s 85ms/step - loss: 0.1767\n",
      "Epoca: 398\n",
      "1056/1056 [==============================] - 90s 85ms/step - loss: 0.1767\n",
      "Epoca: 399\n",
      "1056/1056 [==============================] - 89s 84ms/step - loss: 0.1767\n",
      "Epoca: 400\n",
      "1056/1056 [==============================] - 93s 88ms/step - loss: 0.1767\n",
      "Epoca: 401\n",
      "1056/1056 [==============================] - 84s 79ms/step - loss: 0.1767\n",
      "Epoca: 402\n",
      "1056/1056 [==============================] - 87s 83ms/step - loss: 0.1767\n",
      "Epoca: 403\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1766\n",
      "Epoca: 404\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1767\n",
      "Epoca: 405\n",
      "1056/1056 [==============================] - 91s 86ms/step - loss: 0.1766\n",
      "Epoca: 406\n",
      "1056/1056 [==============================] - 84s 80ms/step - loss: 0.1766\n",
      "Epoca: 407\n",
      "1056/1056 [==============================] - 92s 87ms/step - loss: 0.1766\n",
      "Epoca: 408\n",
      "1056/1056 [==============================] - 87s 83ms/step - loss: 0.1766\n",
      "Epoca: 409\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1766\n",
      "Epoca: 410\n",
      "1056/1056 [==============================] - 92s 87ms/step - loss: 0.1766\n",
      "Epoca: 411\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1766\n",
      "Epoca: 412\n",
      "1056/1056 [==============================] - 87s 83ms/step - loss: 0.1766\n",
      "Epoca: 413\n",
      "1056/1056 [==============================] - 85s 81ms/step - loss: 0.1766\n",
      "Epoca: 414\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1766\n",
      "Epoca: 415\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1766\n",
      "Epoca: 416\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1766\n",
      "Epoca: 417\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1766\n",
      "Epoca: 418\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1765\n",
      "Epoca: 419\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1766\n",
      "Epoca: 420\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1766\n",
      "Epoca: 421\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1765\n",
      "Epoca: 422\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1765\n",
      "Epoca: 423\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1765\n",
      "Epoca: 424\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1765\n",
      "Epoca: 425\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1765\n",
      "Epoca: 426\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1765\n",
      "Epoca: 427\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1764\n",
      "Epoca: 428\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1764\n",
      "Epoca: 429\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1764\n",
      "Epoca: 430\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1765\n",
      "Epoca: 431\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1764\n",
      "Epoca: 432\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1764\n",
      "Epoca: 433\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1763\n",
      "Epoca: 434\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1764\n",
      "Epoca: 435\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1764\n",
      "Epoca: 436\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1765\n",
      "Epoca: 437\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1763\n",
      "Epoca: 438\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1763\n",
      "Epoca: 439\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1764\n",
      "Epoca: 440\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1764\n",
      "Epoca: 441\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1763\n",
      "Epoca: 442\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1763\n",
      "Epoca: 443\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1763\n",
      "Epoca: 444\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1763\n",
      "Epoca: 445\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1763\n",
      "Epoca: 446\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1763\n",
      "Epoca: 447\n",
      "1056/1056 [==============================] - 90s 85ms/step - loss: 0.1763\n",
      "Epoca: 448\n",
      "1056/1056 [==============================] - 85s 81ms/step - loss: 0.1763\n",
      "Epoca: 449\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1763\n",
      "Epoca: 450\n",
      "1056/1056 [==============================] - 88s 84ms/step - loss: 0.1762\n",
      "Epoca: 451\n",
      "1056/1056 [==============================] - 85s 80ms/step - loss: 0.1763\n",
      "Epoca: 452\n",
      "1056/1056 [==============================] - 86s 82ms/step - loss: 0.1762\n",
      "Epoca: 453\n",
      "1056/1056 [==============================] - 89s 84ms/step - loss: 0.1762\n",
      "Epoca: 454\n",
      "1056/1056 [==============================] - 89s 84ms/step - loss: 0.1762\n",
      "Epoca: 455\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1762\n",
      "Epoca: 456\n",
      "1056/1056 [==============================] - 90s 85ms/step - loss: 0.1761\n",
      "Epoca: 457\n",
      "1056/1056 [==============================] - 89s 84ms/step - loss: 0.1762\n",
      "Epoca: 458\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1763\n",
      "Epoca: 459\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1762\n",
      "Epoca: 460\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1761\n",
      "Epoca: 461\n",
      "1056/1056 [==============================] - 87s 82ms/step - loss: 0.1762\n",
      "Epoca: 462\n",
      "1056/1056 [==============================] - 89s 84ms/step - loss: 0.1762\n",
      "Epoca: 463\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1762\n",
      "Epoca: 464\n",
      "1056/1056 [==============================] - 90s 85ms/step - loss: 0.1761\n",
      "Epoca: 465\n",
      "1056/1056 [==============================] - 90s 85ms/step - loss: 0.1761\n",
      "Epoca: 466\n",
      "1056/1056 [==============================] - 91s 86ms/step - loss: 0.1761\n",
      "Epoca: 467\n",
      "1056/1056 [==============================] - 91s 86ms/step - loss: 0.1762\n",
      "Epoca: 468\n",
      "1056/1056 [==============================] - 87s 83ms/step - loss: 0.1761\n",
      "Epoca: 469\n",
      "1056/1056 [==============================] - 87s 82ms/step - loss: 0.1762\n",
      "Epoca: 470\n",
      "1056/1056 [==============================] - 88s 83ms/step - loss: 0.1761\n",
      "Epoca: 471\n",
      "1056/1056 [==============================] - 85s 81ms/step - loss: 0.1760\n",
      "Epoca: 472\n",
      "1056/1056 [==============================] - 86s 82ms/step - loss: 0.1761\n",
      "Epoca: 473\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1761\n",
      "Epoca: 474\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1761\n",
      "Epoca: 475\n",
      "1056/1056 [==============================] - 86s 82ms/step - loss: 0.1761\n",
      "Epoca: 476\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1761\n",
      "Epoca: 477\n",
      "1056/1056 [==============================] - 86s 82ms/step - loss: 0.1761\n",
      "Epoca: 478\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1761\n",
      "Epoca: 479\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1761\n",
      "Epoca: 480\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1760\n",
      "Epoca: 481\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1760\n",
      "Epoca: 482\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1760\n",
      "Epoca: 483\n",
      "1056/1056 [==============================] - 82s 78ms/step - loss: 0.1761\n",
      "Epoca: 484\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1760\n",
      "Epoca: 485\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1760\n",
      "Epoca: 486\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1760\n",
      "Epoca: 487\n",
      "1056/1056 [==============================] - 83s 78ms/step - loss: 0.1760\n",
      "Epoca: 488\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1760\n",
      "Epoca: 489\n",
      "1056/1056 [==============================] - 83s 79ms/step - loss: 0.1760\n",
      "Epoca: 490\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1760\n",
      "Epoca: 491\n",
      "1056/1056 [==============================] - 87s 82ms/step - loss: 0.1759\n",
      "Epoca: 492\n",
      "1056/1056 [==============================] - 100s 95ms/step - loss: 0.1759\n",
      "Epoca: 493\n",
      "1056/1056 [==============================] - 92s 87ms/step - loss: 0.1759\n",
      "Epoca: 494\n",
      "1056/1056 [==============================] - 94s 89ms/step - loss: 0.1759\n",
      "Epoca: 495\n",
      "1056/1056 [==============================] - 97s 92ms/step - loss: 0.1759\n",
      "Epoca: 496\n",
      "1056/1056 [==============================] - 91s 86ms/step - loss: 0.1759\n",
      "Epoca: 497\n",
      "1056/1056 [==============================] - 95s 90ms/step - loss: 0.1758\n",
      "Epoca: 498\n",
      "1056/1056 [==============================] - 99s 94ms/step - loss: 0.1759\n",
      "Epoca: 499\n",
      "1056/1056 [==============================] - 86s 81ms/step - loss: 0.1759\n",
      "Epoca: 500\n",
      "1056/1056 [==============================] - 101s 96ms/step - loss: 0.1759\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "\n",
    "# Se crea la carpeta para una versión del modelo\n",
    "import os\n",
    "ruta_modelo = \"model/v9-1/\"\n",
    "if not os.path.exists(ruta_modelo):\n",
    "  os.mkdir(ruta_modelo)\n",
    "\n",
    "# Cargar modelo\n",
    "#from keras.models import load_model\n",
    "\n",
    "#fileWeights = 'model/v5-1/pesos.h5'\n",
    "#model.load_weights(fileWeights)\n",
    "\n",
    "x = [np.array(encoder_input), np.array(decoder_input)]\n",
    "y = np.array(output_decoded)\n",
    "\n",
    "# Guardamos la arquitectura\n",
    "model.save(ruta_modelo + \"modelo.h5\")\n",
    "\n",
    "# Variable para iteraciones\n",
    "acc = 257\n",
    "while (acc < 500):\n",
    "  print('Epoca:',acc + 1)\n",
    "  model.fit(x,y, epochs = 1, batch_size=32)\n",
    "  model.save_weights(ruta_modelo + \"pesos.h5\")\n",
    "  acc = acc + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1642288770101,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "3_RyvG6wVjqQ",
    "outputId": "2e364bd5-318b-4ecb-93cb-92848143386b"
   },
   "outputs": [],
   "source": [
    "# Guardar \n",
    "import os\n",
    "ruta_modelo = \"model/v9-1/\"\n",
    "if not os.path.exists(ruta_modelo):\n",
    "  os.mkdir(ruta_modelo)\n",
    "\n",
    "model.save(ruta_modelo+\"modelo.h5\")\n",
    "model.save_weights(ruta_modelo+\"pesos.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MIz986QPT8Q2"
   },
   "outputs": [],
   "source": [
    "# Cargar modelo\n",
    "ruta_modelo = \"model/v9-1/\"\n",
    "modelo = \"modelo.h5\"\n",
    "pesos = 'pesos.h5'\n",
    "model.load_weights(ruta_modelo+pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "agG-IjROUYMc"
   },
   "outputs": [],
   "source": [
    "def decode_source(oracion):\n",
    "    \"\"\"\n",
    "    Toma una oración del lenguaje source y la decodifica\n",
    "    \n",
    "    Parámetros\n",
    "    oracion: Oración del lenguaje source en numeros/tokens\n",
    "    \n",
    "    return oración decodificada\n",
    "    \"\"\"\n",
    "    oracion = ' '.join(map(lambda x: source_token_dict_inv[x], oracion)).replace(\" <PAD>\",\"\")\n",
    "    oracion = oracion.replace(\"@@ \",\"\").replace(\" <END>\",\"\").replace(\"<START>\",\"\")\n",
    "    return oracion\n",
    "\n",
    "def decode_translated(oracion):\n",
    "    \"\"\"\n",
    "    Toma una oración del lenguaje target y la decodifica\n",
    "    \n",
    "    Parámetros\n",
    "    oracion: Oración del lenguaje target en numeros/tokens\n",
    "    \n",
    "    return oración decodificada\n",
    "    \"\"\"\n",
    "    oracion = ' '.join(map(lambda x: target_token_dict_inv[x], oracion)).replace(\" <PAD>\",\"\")\n",
    "    oracion = oracion.replace(\"@@ \",\"\").replace(\" <END>\",\"\").replace(\"<START>\",\"\")\n",
    "    return oracion\n",
    "    \n",
    "def translate(indice):\n",
    "    \"\"\"\n",
    "    Traducción de una oración\n",
    "    \n",
    "    Parámetros\n",
    "    indice: indice en la lista de test \n",
    "    \"\"\"\n",
    "    sentence = encoder_input_test[indice]\n",
    "    original = decoder_input_test[indice]\n",
    "    decoded = decode(\n",
    "      model, \n",
    "      sentence, \n",
    "      start_token = target_token_dict['<START>'],\n",
    "      end_token = target_token_dict['<END>'],\n",
    "      pad_token = target_token_dict['<PAD>']\n",
    "    )\n",
    "\n",
    "    print('Frase original:\\t\\t {}'.format(decode_source(sentence)))\n",
    "    print('Traducción:\\t\\t {}'.format(decode_translated(decoded)))\n",
    "    print('Traducción original:\\t {}'.format(decode_translated(original)))\n",
    "    \n",
    "def translate_bleu(indice):\n",
    "    \"\"\"\n",
    "    Traducción de una oración\n",
    "    \n",
    "    Parámetros\n",
    "    indice: indice en la lista de test \n",
    "    \n",
    "    return oración-traducción predecida y original\n",
    "    \"\"\"\n",
    "    sentence = encoder_input_test[indice]\n",
    "    original = decoder_input_test[indice]\n",
    "    decoded = decode(\n",
    "      model, \n",
    "      sentence, \n",
    "      start_token = target_token_dict['<START>'],\n",
    "      end_token = target_token_dict['<END>'],\n",
    "      pad_token = target_token_dict['<PAD>']\n",
    "    )\n",
    "\n",
    "    #print('Traducción:\\t\\t {}'.format())\n",
    "    #print('Traducción original:\\t {}'.format())\n",
    "    \n",
    "    return decode_translated(decoded), decode_translated(original)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1642212600675,
     "user": {
      "displayName": "Yair Vazquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhR4dAyrqsKS6WDqVTYthlcL0b5GfvGr8tQvsFdA=s64",
      "userId": "08287527673782438275"
     },
     "user_tz": 360
    },
    "id": "7ypsr2IZe8bZ",
    "outputId": "e3638ab6-a584-4b0c-c1c4-d066c0fbb609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase original:\t\t  zeki kitektinemij kampa onkak uan kinamakaj zakapoaztan uan no cuezalan\n",
      "Traducción:\t\t  unos pájaros también hay en donde hay en los palos y se puede se puede haber\n",
      "Traducción original:\t  algunos lo andan cortando donde lo hay y lo venden en zacapoaxtla y también en cuetzalan\n"
     ]
    }
   ],
   "source": [
    "translate(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos cual es el valor con más tokens\n",
    "maximo = 0\n",
    "long = 0\n",
    "for i in range(len(encoder_input_test)):\n",
    "    val = len(encoder_input_test[i])-encoder_input_test[i].count(0)\n",
    "    if val > long:\n",
    "        maximo = i\n",
    "        long = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3752/3752 [2:37:41<00:00,  2.52s/it]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Variables globales\n",
    "traducciones = list()\n",
    "originales = list()\n",
    "\n",
    "for i in tqdm(range(len(encoder_input_test))):\n",
    "    tra, orig = translate_bleu(i)\n",
    "    traducciones.append(tra)\n",
    "    originales.append(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU = 2.59 18.2/4.0/1.2/0.5 (BP = 1.000 ratio = 1.065 hyp_len = 88840 ref_len = 83431)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable global\n",
    "bleu = BLEU()\n",
    "bleu.corpus_score(traducciones, [originales])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Traductor.ipynb",
   "provenance": [
    {
     "file_id": "1Zmwp1Y2IHHCiHOiLHAuYINIqg7IEbrhE",
     "timestamp": 1642213214548
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
